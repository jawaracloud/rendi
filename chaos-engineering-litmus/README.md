# Chaos Engineering with Litmus

A production-ready chaos engineering platform for testing Kubernetes application resilience using Litmus Chaos Engineering.

![Chaos Engineering Architecture](https://via.placeholder.com/800x400/1a1a2e/E74C3C?text=Chaos+Engineering+with+Litmus)

## Overview

This project demonstrates how to practice chaos engineering in a cloud-native way using Litmus. Chaos engineering is the discipline of experimenting on a system to build confidence in its capability to withstand turbulent conditions in production.

### What is Litmus?

Litmus is an open-source chaos engineering platform that helps SREs and developers:
- **Find weaknesses** before they cause outages
- **Validate resilience** patterns in production-like environments
- **Automate chaos** as part of CI/CD pipelines
- **Measure recovery** times and system behavior

## Quick Start

### Prerequisites

- Kubernetes cluster (1.24+)
- kubectl configured
- Helm 3.x

### 1. Install Litmus

```bash
# Add Litmus Helm repository
helm repo add litmuschaos https://litmuschaos.github.io/litmus-helm/
helm repo update

# Install Litmus ChaosCenter
helm install litmus litmuschaos/litmus --namespace litmus --create-namespace

# Wait for pods to be ready
kubectl wait --for=condition=Ready pods --all -n litmus --timeout=300s

# Port-forward to access ChaosCenter
kubectl port-forward svc/litmus-frontend-service 9091:9091 -n litmus
```

Access ChaosCenter at: http://localhost:9091

**Default credentials:**
- Username: `admin`
- Password: `litmus`

### 2. Install Chaos Infrastructure

```bash
# Install ChaosCenter Agent on your cluster
kubectl apply -f manifests/litmus-agent.yaml
```

### 3. Run Your First Experiment

```bash
# Apply a simple pod-delete experiment
kubectl apply -f experiments/pod-delete.yaml

# Watch the experiment
kubectl get chaosengine -n litmus -w
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Litmus Architecture                          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   ChaosCenter   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚     Chaos Agent              â”‚  â”‚
â”‚  â”‚   (Control)     â”‚         â”‚     (Execution)              â”‚  â”‚
â”‚  â”‚                 â”‚         â”‚                              â”‚  â”‚
â”‚  â”‚ â€¢ UI/Portal     â”‚         â”‚ â€¢ Workflow Engine            â”‚  â”‚
â”‚  â”‚ â€¢ Workflow Mgmt â”‚         â”‚ â€¢ Experiment Runner          â”‚  â”‚
â”‚  â”‚ â€¢ Scheduling    â”‚         â”‚ â€¢ Probes & Monitoring        â”‚  â”‚
â”‚  â”‚ â€¢ Reporting     â”‚         â”‚ â€¢ Event Recording            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â”‚                       â”‚
â”‚                                         â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Target Applications                        â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚   â”‚
â”‚  â”‚  â”‚ Frontend â”‚  â”‚ Backend  â”‚  â”‚ Database â”‚             â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Chaos Experiments

### 1. Pod Failure (`pod-delete`)

Simulates sudden pod termination to test Kubernetes self-healing.

**Use case:** Verify deployment replica sets recreate pods correctly.

```yaml
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosEngine
metadata:
  name: frontend-pod-delete
  namespace: litmus
spec:
  appinfo:
    appns: 'default'
    applabel: 'app=frontend'
    appkind: 'deployment'
  chaosServiceAccount: pod-delete-sa
  monitoring: true
  jobCleanUpPolicy: 'delete'
  experiments:
    - name: pod-delete
      spec:
        components:
          env:
            - name: TOTAL_CHAOS_DURATION
              value: '30'
            - name: CHAOS_INTERVAL
              value: '10'
            - name: FORCE
              value: 'false'
            - name: PODS_AFFECTED_PERC
              value: '50'
```

**Run:**
```bash
kubectl apply -f experiments/pod-delete.yaml
```

### 2. Network Latency (`network-latency`)

Introduces network delays to test timeout handling and retry logic.

**Use case:** Verify microservices handle network degradation gracefully.

```yaml
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosEngine
metadata:
  name: api-network-latency
  namespace: litmus
spec:
  appinfo:
    appns: 'default'
    applabel: 'app=api'
    appkind: 'deployment'
  chaosServiceAccount: network-latency-sa
  monitoring: true
  experiments:
    - name: network-latency
      spec:
        components:
          env:
            - name: TARGET_CONTAINER
              value: 'api'
            - name: NETWORK_INTERFACE
              value: 'eth0'
            - name: LIB_IMAGE
              value: 'litmuschaos/go-runner:latest'
            - name: TC_IMAGE
              value: 'gaiadocker/iproute2'
            - name: NETWORK_LATENCY
              value: '2000'  # 2 seconds
            - name: TOTAL_CHAOS_DURATION
              value: '60'
```

**Run:**
```bash
kubectl apply -f experiments/network-latency.yaml
```

### 3. CPU Hog (`cpu-hog`)

Consumes CPU resources to test auto-scaling and resource limits.

**Use case:** Verify HPA scales pods when CPU usage spikes.

```yaml
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosEngine
metadata:
  name: backend-cpu-hog
  namespace: litmus
spec:
  appinfo:
    appns: 'default'
    applabel: 'app=backend'
    appkind: 'deployment'
  chaosServiceAccount: cpu-hog-sa
  experiments:
    - name: cpu-hog
      spec:
        components:
          env:
            - name: CPU_CORES
              value: '2'
            - name: TOTAL_CHAOS_DURATION
              value: '120'
            - name: CHAOS_INTERVAL
              value: '10'
```

### 4. Memory Hog (`memory-hog`)

Consumes memory to test OOM handling and resource quotas.

**Use case:** Verify pods restart gracefully when memory limits exceeded.

```yaml
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosEngine
metadata:
  name: worker-memory-hog
  namespace: litmus
spec:
  appinfo:
    appns: 'default'
    applabel: 'app=worker'
    appkind: 'deployment'
  chaosServiceAccount: memory-hog-sa
  experiments:
    - name: memory-hog
      spec:
        components:
          env:
            - name: MEMORY_CONSUMPTION
              value: '500'  # MB
            - name: TOTAL_CHAOS_DURATION
              value: '90'
```

### 5. Disk Fill (`disk-fill`)

Fills disk space to test storage management and cleanup.

**Use case:** Verify log rotation and temp file cleanup work correctly.

```yaml
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosEngine
metadata:
  name: database-disk-fill
  namespace: litmus
spec:
  appinfo:
    appns: 'default'
    applabel: 'app=database'
    appkind: 'statefulset'
  chaosServiceAccount: disk-fill-sa
  experiments:
    - name: disk-fill
      spec:
        components:
          env:
            - name: FILL_PERCENTAGE
              value: '80'
            - name: TOTAL_CHAOS_DURATION
              value: '60'
```

## Chaos Workflows

### GameDay Scenario: Complete Infrastructure Failure

Run multiple experiments in sequence to simulate a cascading failure:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: gameday-cascading-failure
  namespace: litmus
spec:
  entrypoint: chaos-sequence
  templates:
    - name: chaos-sequence
      steps:
        - - name: network-degradation
            template: network-latency
        - - name: database-stress
            template: cpu-hog-db
        - - name: pod-failures
            template: pod-delete-frontend
    
    - name: network-latency
      container:
        image: litmuschaos/litmus-checker:latest
        args:
          - -file=/tmp/network-latency.yaml
    
    - name: cpu-hog-db
      container:
        image: litmuschaos/litmus-checker:latest
        args:
          - -file=/tmp/cpu-hog.yaml
    
    - name: pod-delete-frontend
      container:
        image: litmuschaos/litmus-checker:latest
        args:
          - -file=/tmp/pod-delete.yaml
```

## Observability

### Monitoring with Prometheus

Litmus exports metrics to Prometheus:

```yaml
# ServiceMonitor for Litmus
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: litmus-metrics
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: litmus
  endpoints:
    - port: http
      interval: 30s
```

**Key Metrics:**
- `litmuschaos_passed_experiments_total` - Successful experiments
- `litmuschaos_failed_experiments_total` - Failed experiments
- `litmuschaos_injected_experiments_total` - Total injected
- `litmuschaos_experiment_duration_seconds` - Experiment duration

### Grafana Dashboards

Import the Litmus dashboard (ID: 13725) for visualization:

```bash
# Port-forward Grafana
kubectl port-forward svc/grafana 3000:3000 -n monitoring

# Dashboard URL: http://localhost:3000/d/litmus-dashboard
```

**Dashboards show:**
- Experiment execution history
- Pass/fail rates over time
- System stability scores
- Recovery time measurements

## Case Study: E-commerce Platform Resilience Testing

### The Challenge

A growing e-commerce platform experienced several outages:
- **Black Friday 2022**: Database connection pool exhaustion caused 45-minute outage
- **Flash Sale**: Network latency spike cascaded to payment service failures
- **Infrastructure Migration**: Unnoticed pod anti-affinity rules caused AZ failure

**Impact:** $2.3M revenue loss, customer trust erosion

### The Solution

Implemented Litmus chaos engineering with systematic resilience testing:

**Phase 1: Weekly GameDays (Month 1-3)**
- Pod failures in staging environment
- Network latency injection
- CPU/memory stress tests
- Database connection drops

**Phase 2: Production Chaos (Month 4-6)**
- Limited blast radius experiments
- Business hours testing with monitoring
- Automated rollback procedures
- On-call team readiness drills

**Phase 3: CI/CD Integration (Month 7-12)**
- Pre-deployment chaos gates
- Automated regression testing
- Stability score thresholds
- Chaos as a quality gate

### Chaos Experiments Implemented

```yaml
# Weekly Chaos Schedule
Monday 10:00 AM: pod-delete (frontend) - 15 min
Tuesday 2:00 PM: network-latency (api) - 30 min
Wednesday 10:00 AM: cpu-hog (backend) - 20 min
Thursday 2:00 PM: memory-hog (cache) - 25 min
Friday 10:00 AM: disk-fill (logs) - 10 min

# Monthly GameDay (4 hours)
Cascading failure simulation:
1. Network partition between services
2. Database primary failover
3. Cache cluster node failures
4. CDN origin unavailability
```

### Results After 12 Months

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Mean Time to Recovery (MTTR) | 45 min | 8 min | **82% faster** |
| Production Incidents | 12/quarter | 3/quarter | **75% reduction** |
| Failed Deployment Rollbacks | 15% | 4% | **73% reduction** |
| Customer-Impacting Outages | 4/year | 0/year | **100% elimination** |
| Confidence Score (Team Survey) | 4.2/10 | 8.7/10 | **+107%** |

### Key Discoveries

**Critical Weaknesses Found:**
1. **Circuit breaker misconfiguration**: 30s timeout too long for 5s SLA
2. **Missing retry backoff**: Immediate retries overwhelmed recovering services
3. **Insufficient health checks**: Liveness probes didn't catch partial failures
4. **Resource limits too tight**: No headroom for traffic spikes

**Resilience Patterns Validated:**
1. âœ… Graceful degradation (fallback to cached data)
2. âœ… Automatic failover (database replica promotion)
3. âœ… Rate limiting (prevented cascade failures)
4. âœ… Bulkhead isolation (contained blast radius)

### Financial Impact

**Costs:**
- Litmus infrastructure: $200/month
- Engineering time: 2 hours/week
- Total annual cost: ~$15,000

**Savings:**
- Prevented outages: $2.3M (based on previous incidents)
- Reduced MTTR cost: $180K (engineer time)
- Improved deployment confidence: $95K (reduced rollback waste)

**ROI: 20,600%** ðŸŽ¯

## Best Practices

### 1. Start Small, Expand Gradually

```bash
# Month 1: Staging only
kubectl apply -f experiments/staging/

# Month 3: Production (limited scope)
kubectl apply -f experiments/production/non-critical/

# Month 6: Production (full coverage)
kubectl apply -f experiments/production/
```

### 2. Define Clear Abort Conditions

Always have automatic stop conditions:

```yaml
spec:
  experiments:
    - name: pod-delete
      spec:
        probe:
          - name: "health-check"
            type: "httpProbe"
            mode: "Continuous"
            runProperties:
              probeTimeout: "5s"
              retry: 2
              interval: "5s"
              probePollingInterval: "2s"
            httpProbe/inputs:
              url: "http://frontend:8080/health"
              insecureSkipVerify: false
              method:
                get:
                  criteria: "=="
                  responseCode: "200"
```

### 3. Measure Before and After

```bash
# Baseline metrics
kubectl top pods -n default > baseline-metrics.txt

# Run experiment
kubectl apply -f experiments/cpu-hog.yaml

# Compare metrics
kubectl top pods -n default > chaos-metrics.txt
diff baseline-metrics.txt chaos-metrics.txt
```

### 4. Document Learnings

Create a chaos engineering runbook:

```markdown
# Chaos Engineering Runbook

## Experiment: Pod Delete - Frontend
Date: 2024-01-15
Engineer: Sarah Chen

### Hypothesis
Frontend deployment will recover within 60s with zero errors.

### Results
- Recovery time: 45s âœ…
- Error rate: 0.2% (within SLA) âœ…
- Customer impact: None âœ…

### Learnings
- Liveness probe (10s interval) could be faster
- Consider reducing to 5s for quicker detection

### Action Items
- [ ] Update probe interval
- [ ] Document recovery procedure
```

## Integration with CI/CD

### GitOps with ArgoCD

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: chaos-experiments
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/jawaracloud/rendi.git
    targetRevision: main
    path: chaos-engineering-litmus/experiments
  destination:
    server: https://kubernetes.default.svc
    namespace: litmus
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

### Pre-Deployment Chaos Gate

```yaml
# .github/workflows/chaos-gate.yaml
name: Chaos Gate
on:
  pull_request:
    branches: [main]

jobs:
  chaos-test:
    runs-on: ubuntu-latest
    steps:
      - name: Run Litmus Experiments
        run: |
          kubectl apply -f experiments/ci-gate/
          kubectl wait --for=condition=Completed chaosengine -n litmus --timeout=300s
      
      - name: Verify Results
        run: |
          kubectl get chaosresult -n litmus -o json | jq '.items[].status.experimentStatus.verdict'
```

## Troubleshooting

### Common Issues

**1. Experiment stuck in "Awaiting" state**
```bash
# Check if target pods exist
kubectl get pods -n default -l app=frontend

# Verify service account permissions
kubectl auth can-i create pods -n default --as=system:serviceaccount:litmus:pod-delete-sa
```

**2. Metrics not appearing in Prometheus**
```bash
# Check if ServiceMonitor is created
kubectl get servicemonitor -n monitoring

# Verify metric endpoints
curl http://litmus-agent-metrics:8080/metrics
```

**3. ChaosCenter not accessible**
```bash
# Check pod status
kubectl get pods -n litmus

# Check logs
kubectl logs -l app=litmus-frontend -n litmus

# Verify service
kubectl get svc litmus-frontend-service -n litmus
```

## Future Enhancements

- [ ] **AI-Driven Chaos**: ML models to predict optimal chaos timing
- [ ] **Cost-Aware Chaos**: Consider spot instance pricing when planning
- [ ] **Multi-Cluster Chaos**: Cross-region failure simulation
- [ ] **Custom Probes**: Application-specific health checks
- [ ] **Compliance Integration**: SOC2/ISO27001 evidence collection

## References

- **Litmus Documentation**: https://docs.litmuschaos.io/
- **Chaos Engineering Book**: https://www.oreilly.com/library/view/chaos-engineering/9781492043864/
- **Principles of Chaos**: https://principlesofchaos.org/
- **Chaos Monkey**: https://netflix.github.io/chaosmonkey/

## Contributing

This project follows Jawaracloud conventions:
- Each experiment must include a README
- Document real-world case studies
- Include troubleshooting guides
- Provide working examples

## License

MIT - See root directory LICENSE

## GitHub

Complete source code:
https://github.com/jawaracloud/rendi/tree/main/chaos-engineering-litmus
